[DEFAULT]
beam_size = 10
batch_size = 2
epoch = 5
lr = 5e-5
dropout = 0.01
weight_lm = 0.2

[model]
save_name = model_codet5p
embed_dim = 768
max_positions = 1024

[model.src_encoder]
in_channels = 192
out_channels = 5
kernel_size = 1
; src_encoder_convolutions = ((192, 5),) * 1
; ctx_encoder_convolutions = ((768, 5),) * 1
; decoder_convolutions = ((192, 5),) * 1

[model.ctx_encoder]
in_channels = 768
out_channels = 5
kernel_size = 1

[model.decoder]
in_channels = 192
out_channels = 5
kernel_size = 1

[file.path]
log_path  = logs\output.log
vocab_path = data\vocabulary\vocabulary.txt
models_path = data\models

train_file_path = data\data\training_src.txt # 这里不用BPE是因为现在的方法里已经有代码分词了
valid_file_path = data\data\validation_src.txt

bug_file_path =  data\data\validation_src.txt
patch_files_path =  data\data\validation_src.txt

identifier_txt_file = data\data\identifier.txt
identifier_token_file = data\data\identifier.tokens


[train]
model = codet5-plus
train = true
valid = false

model_idx = 1

[generat]
dataset = quixbugs
model = codet5-plus

model_idx = 1